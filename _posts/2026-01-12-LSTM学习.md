---
layout:     post
title:      LSTM 学习
subtitle:   		
date:       2026-01-22
author:     Alessia
header-img: img/LSTM.jpg
catalog: 	  true
tags:
    - 学习资料
---

## 〇、总则
基于[李宏毅2017机器学习课程](https://speech.ee.ntu.edu.tw/~hylee/ml/2017-spring.php)
### Multiple-layer LSTM Structure

回顾RNN的结构

![alt text](/img-post/LSTM_img/RNN.png)

Long short-term memory结构如下：We can see that 同等数量units 参数量是RNN的4倍(一个很重要的点)

![alt text](/img-post/LSTM_img/LSTM.png)

LSTM网络则是将每一个隐藏层单元替换为一个LSTM unit。
多层LSTM网络结构如下：除了输入序列x外，隐藏层输出h，使用Extension “peephole”，memory cell输出也作为下一时刻的输入

![alt text](/img-post/LSTM_img/multiLayerLSTM.png)

### 训练
BPTT(backpropagation through time)，考虑时间信息

![alt text](/img-post/LSTM_img/BPTT.png)

RNN traning比较困难，容易梯度爆炸：

![alt text](/img-post/LSTM_img/gradient_explosion.png)

避免梯度爆炸的technique：当梯度大于15时截断（发明人的方法）

<br><br/>

## 一、实现

```python

```


## 二、应用
```python
class GridCellsRNNCell(snt.RNNCore):
	def __init__(self,target_ensembles,nh_lstm,nh_bottleneck,nh_embed=None,...
	dropoutrates_bottleneck=None):


```

### batch size的影响

loss function is across entropy:

$-\sum_i^{NPC} P(y_i)logP(\hat y_i)-\sum_j^{NHDC} P(z_j)P(\hat z_j)$

NPC=256, NHDC=32. This will cause loss pc 
















